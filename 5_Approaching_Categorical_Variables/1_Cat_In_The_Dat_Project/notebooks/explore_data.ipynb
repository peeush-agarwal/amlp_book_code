{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(train_data['target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, from above plot as well as counts, that target distribution is **skewed**. Because of skewed distribution, we shouldn't use `accuracy_score` as the evaluation metric. We're going to use *Area under ROC curve (AUC)* evaluation metric. We can argue to use *Precision* or *Recall* too, but AUC combines both Precision as well as Recall, therefore, we choose AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "As we can make out from the column names, there are:\n",
    "+ 05 Binary variables\n",
    "+ 10 Nominal variables\n",
    "+ 06 Ordinal variables\n",
    "+ 02 Cyclic variables\n",
    "+ 01 Target variable (It is a label or actual target column, not a feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe data in first 10 rows, the dataset columns have numbers as well as strings in the data. We can also observe that there are `NaN` values as well. But the computers or machines only understand numbers while training or evaluating the model, therefore, there is a need to transform strings into numbers, so that we can make machines learn patterns from this dataset to use them for inference in later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take `ord_2` column from the dataset, it has different categories which are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ord_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for above type of columns, we can define dictionary to map each category as a number and then replace each category in column to the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_data['ord_2'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {key:idx for idx,key in enumerate(train_data['ord_2'].value_counts().index)}\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[:, 'ord_2'] = train_data['ord_2'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ord_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "Encoding where each category is encoded as a numerical label. Example, what we did in `ord_2` column transformation.\n",
    "\n",
    "We can do same thing by using `LabelEncoder` in `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in ord_2 column\n",
    "train_data.loc[:,'ord_2'] = train_data.ord_2.fillna('NONE')\n",
    "train_data['ord_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoding\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the data\n",
    "lbl_enc.fit(train_data['ord_2'].values)\n",
    "\n",
    "train_data.loc[:, 'ord_2'] = lbl_enc.transform(train_data['ord_2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ord_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *LabelEncoder doesn't handle NaN values and therefore we need to fill NaN values before fit and transform of column*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LabelEncoder** can be used directly in tree-based models:\n",
    "+ Decision trees\n",
    "+ Random forest\n",
    "+ Extra trees\n",
    "+ Boosted trees:\n",
    "  + XGBoost\n",
    "  + GBM\n",
    "  + LightGBM\n",
    "  \n",
    "This type of encoding cannot be used in linear models, SVM or Neural networks as they expect data to be normalized (or standardized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear models mentioned above, we can binarize the data in columns, as shown below:\n",
    "\n",
    "Category | Label | Bin_Label_0 | Bin_Label_1 | Bin_Label_2\n",
    "--- | --- | --- | --- | ---\n",
    "Freezing | 0 | 0 | 0 | 0\n",
    "Warm | 1 | 0 | 0 | 1\n",
    "Cold | 2 | 0 | 1 | 0\n",
    "Boiling Hot | 3 | 0 | 1 | 1\n",
    "Hot | 4 | 1 | 0 | 0\n",
    "Lava Hot | 5 | 1 | 0 | 1\n",
    "NONE | 6 | 1 | 1 | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had only 7 categories, we could represent one column to 3 binarized columns. But as you can see that if we have huge number of categories, we'll have large number of binarized columns. And in that case we will have data sparsely populated (i.e. the number of 1s will be very less).\n",
    "\n",
    "If we store binarized variables in **sparse format** i.e. store only values that are relevant (which are 1s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between densely stored vs sparsed format storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example stored as dense matrix\n",
    "import numpy as np\n",
    "\n",
    "example = np.array(\n",
    "[\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "print(example.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it takes 72 bytes to store the data in dense format. Let's take a look if we store the data in sparse format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "example = np.array(\n",
    "[\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "\n",
    "print(sparse_example.data.nbytes)\n",
    "\n",
    "# The total size of sparse csr matrix is the sum of three values\n",
    "print(sparse_example.data.nbytes + \n",
    "     sparse_example.indptr.nbytes + \n",
    "     sparse_example.indices.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that space taken by sparse format storage is less compared to dense format storage. This difference can be very large in case of larger arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense size       : 8000000000\n",
      "Sparse size      : 399981480\n",
      "Sparse total size: 600012224\n",
      "Dense size (GB)       : 8.0\n",
      "Sparse size (GB)      : 0.39998148\n",
      "Sparse total size (GB): 0.600012224\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "bytes_to_gb = lambda x: x/(10**9)\n",
    "sp_mat_total_size = lambda spm: spm.data.nbytes + spm.indptr.nbytes + spm.indices.nbytes\n",
    "\n",
    "n_rows = 10000\n",
    "n_cols = 100000\n",
    "\n",
    "# Let's build a dense matrix with only 5% 1s\n",
    "example = np.random.binomial(n=1, p=0.05, size=(n_rows, n_cols))\n",
    "\n",
    "# print dense matrix size\n",
    "print('Dense size       :', example.nbytes)\n",
    "\n",
    "sp_mat = sparse.csr_matrix(example)\n",
    "\n",
    "# print sparse matrix size and total_size\n",
    "print('Sparse size      :', sp_mat.data.nbytes)\n",
    "print('Sparse total size:', sp_mat_total_size(sp_mat))\n",
    "\n",
    "\n",
    "print('Dense size (GB)       :', bytes_to_gb(example.nbytes))\n",
    "print('Sparse size (GB)      :', bytes_to_gb(sp_mat.data.nbytes))\n",
    "print('Sparse total size (GB):', bytes_to_gb(sp_mat_total_size(sp_mat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can compare the storage difference in case of Dense vs Sparse format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "It is a kind of binary encoding but representation of label is not binary representation. It takes even less memory compared to binarized encoding.\n",
    "\n",
    "Let's one-hot encode our ord_2 categories, which can be represented as:\n",
    "\n",
    "Category | Category_Freezing | Category_Warm | Category_Cold | Category_Boiling_Hot | Category_Hot | Category_Lava_Hot | Category_NONE\n",
    "--- | --- | --- | --- | --- | --- | --- | ---\n",
    "Freezing | 1 | 0 | 0 | 0 | 0 | 0 | 0\n",
    "Warm | 0 | 1 | 0 | 0 | 0 | 0 | 0\n",
    "Cold | 0 | 0 | 1 | 0 | 0 | 0 | 0\n",
    "Boiling Hot | 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
    "Hot | 0 | 0 | 0 | 0 | 1 | 0 | 0\n",
    "Lava Hot | 0 | 0 | 0 | 0 | 0 | 1 | 0\n",
    "NONE | 0 | 0 | 0 | 0 | 0 | 0 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary to handle categorical variables in any ML project\n",
    "\n",
    "Whenever you get categorical variables, follow these steps:\n",
    "+ Fill the `NaN` values\n",
    "+ Convert them to integers by applying Label Encoding using `LabelEncoder` of scikit-learn or by using a mapping dictionary.\n",
    "+ Create one-hot encoding using `OneHotEncoder` from scikit-learn package. (You can skip binarization)\n",
    "+ Go for ML model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle \"Rare\" or \"Unknown\" categories\n",
    "\n",
    "In real-world problems, we can have a situation where training dataset has a column with similar number of categories. After training the model, in real-time we find a new category in the same column, then our model will throw an error because of unseen category while training. This model is not robust and we need to take care of these situations.\n",
    "\n",
    "We can introduce \"Rare\" category which is a category not seen very often and can include many different categories.\n",
    "If we have a fixed test set, we can add our test set to training set to know about the categories in a given feature. This is similar to semi-supervised learning in which we use data which is not available for training to improve the model. This will also take care of rare values that appear very less number of times in training data but are in abundance in test data. Our model will be more robust. \n",
    "\n",
    "To make sure above model doesn't overfit, we design our cross-validation in such a way that it replicates the prediction process when we run our model on test data, then it never going to overfit.\n",
    "\n",
    "Let's understand it better by going through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "\n",
    "# Create a fake target column in test_data as it doesn't exists already\n",
    "test_data['target'] = -1\n",
    "\n",
    "# concatenate both training and test datasets\n",
    "data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "\n",
    "# Make a list of features we're interested in. Skip 'id' and 'target' columns\n",
    "features = [col for col in train_data.columns if col not in ['id', 'target']]\n",
    "\n",
    "for feat in features:\n",
    "    # Create an object of LabelEncoder for each feature\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    \n",
    "    # Fill NaN values\n",
    "    # Since its categorical data, we fillna with a string and we convert all the data type to string. \n",
    "    # So, no matter its int or float, its converted to string type but categorical.\n",
    "    temp_col = data[feat].fillna('NONE').astype(str).values\n",
    "    \n",
    "    # Fit_Transform the dataset as it is the complete dataset. Otherwise fit here and transform the unseen data\n",
    "    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
    "    \n",
    "# Split the data to train and test again\n",
    "train_data = data[data['target'] != -1].reset_index(drop=True)\n",
    "test_data = data[data['target'] == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>nom_4</th>\n",
       "      <th>nom_5</th>\n",
       "      <th>nom_6</th>\n",
       "      <th>nom_7</th>\n",
       "      <th>nom_8</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1060</td>\n",
       "      <td>1014</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>210</td>\n",
       "      <td>359</td>\n",
       "      <td>27</td>\n",
       "      <td>69</td>\n",
       "      <td>2113</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>151</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>861</td>\n",
       "      <td>694</td>\n",
       "      <td>90</td>\n",
       "      <td>102</td>\n",
       "      <td>1400</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>477</td>\n",
       "      <td>241</td>\n",
       "      <td>51</td>\n",
       "      <td>171</td>\n",
       "      <td>2168</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>361</td>\n",
       "      <td>183</td>\n",
       "      <td>151</td>\n",
       "      <td>1748</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>1060</td>\n",
       "      <td>138</td>\n",
       "      <td>93</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>181</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>645</td>\n",
       "      <td>1223</td>\n",
       "      <td>25</td>\n",
       "      <td>168</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>159</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>277</td>\n",
       "      <td>124</td>\n",
       "      <td>170</td>\n",
       "      <td>1465</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1004</td>\n",
       "      <td>64</td>\n",
       "      <td>138</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>754</td>\n",
       "      <td>1388</td>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>1086</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bin_0  bin_1  bin_2  bin_3  bin_4  nom_0  nom_1  nom_2  nom_3  nom_4  \\\n",
       "0   0      0      0      0      0      0      3      5      3      6      0   \n",
       "1   1      1      1      0      0      2      3      4      0      5      4   \n",
       "2   2      0      1      0      0      0      3      1      3      0      0   \n",
       "3   3      2      0      0      0      0      3      0      3      3      4   \n",
       "4   4      0      2      0      2      0      3      6      3      2      1   \n",
       "5   5      0      2      1      2      0      3      6      4      1      0   \n",
       "6   6      0      0      0      0      0      3      6      3      2      0   \n",
       "7   7      0      0      1      2      0      3      6      0      3      0   \n",
       "8   8      0      0      0      0      0      0      2      3      6      2   \n",
       "9   9      0      0      2      0      2      3      2      3      3      4   \n",
       "\n",
       "   nom_5  nom_6  nom_7  nom_8  nom_9  ord_0  ord_1  ord_2  ord_3  ord_4  \\\n",
       "0   1060   1014     87      1     27      2      0      3      3     21   \n",
       "1    210    359     27     69   2113      2      2      6      5     24   \n",
       "2    861    694     90    102   1400      2      4      2     14     16   \n",
       "3    477    241     51    171   2168      0      5      4      1      2   \n",
       "4    556    361    183    151   1748      2      2      1      8      2   \n",
       "5    767   1060    138     93     59      1      1      3      2     17   \n",
       "6    645   1223     25    168    692      0      2      1      3     18   \n",
       "7     83    277    124    170   1465      2      1      1      2     25   \n",
       "8      6   1004     64    138   1400      0      5      0      3     13   \n",
       "9    754   1388     84      2   1086      2      0      4     14      8   \n",
       "\n",
       "   ord_5  day  month  target  \n",
       "0     57    5      5       0  \n",
       "1    151    6      9       0  \n",
       "2    106    4     11       0  \n",
       "3     46    2      5       0  \n",
       "4     51    4      3       0  \n",
       "5    181    2      6       0  \n",
       "6    159    4      8       0  \n",
       "7     55    0      0       0  \n",
       "8    137    5      5       0  \n",
       "9     51    0     10       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The above trick can work when we have test set available. What if we're working on Real Time problem where we don't have test set available. In these cases we can use \"Unknown\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    39978\n",
       "P    37890\n",
       "Y    36657\n",
       "A    36633\n",
       "R    33045\n",
       "U    32897\n",
       "M    32504\n",
       "X    32347\n",
       "C    32112\n",
       "H    31189\n",
       "Q    30145\n",
       "T    29723\n",
       "O    25610\n",
       "B    25212\n",
       "E    21871\n",
       "K    21676\n",
       "I    19805\n",
       "D    17284\n",
       "F    16721\n",
       "W     8268\n",
       "Z     5790\n",
       "S     4595\n",
       "G     3404\n",
       "V     3107\n",
       "J     1950\n",
       "L     1657\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider ord_4 column. \n",
    "\n",
    "train_data['ord_4'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "G        3404\n",
       "V        3107\n",
       "J        1950\n",
       "L        1657\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fill NaN values\n",
    "train_data['ord_4'].fillna('NONE').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that there are ~18K NaN values in the column.\n",
    "\n",
    "Now, we can define our criteria for \"rare\" or \"unknown\" category. Let's assume that occurrence of category less than 2000 will be considered as \"rare\". We can observe from above that **J and L** categories falls in \"rare\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "RARE     3607\n",
       "G        3404\n",
       "V        3107\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[:, 'ord_4'] = train_data.ord_4.fillna('NONE')\n",
    "\n",
    "train_data.loc[train_data['ord_4'].value_counts()[train_data['ord_4']].values < 2000, 'ord_4'] = 'RARE'\n",
    "train_data['ord_4'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now, when it comes to test data, all the new, unseen categories will be mapped to \"RARE\", and all missing values will be mapped to \"NONE\".\n",
    "\n",
    "This approach will ensure that the model works in a live setting, even, if you have new categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
