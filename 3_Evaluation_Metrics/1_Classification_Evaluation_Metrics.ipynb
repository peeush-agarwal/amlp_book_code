{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics in Binary classification problems\n",
    "\n",
    "We need some kind of evaluation metric to evaluate how good our trained model is doing on unseen data (validation set). Following are the metrics which are commonly used for classification problems:\n",
    "\n",
    "1. Accuracy score\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. Area under ROC (Receiver Operating characteristics) or AUC\n",
    "5. Log loss\n",
    "\n",
    "Knowing above evaluation metrics is good, but we should also know when to use which metric. These metrics usage depend on the problem and mainly target we're trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider, a **binary classification problem** i.e. *the target variable is divided into 2 classes*. An example could be a classification problem for given chest X-ray images to classify if there is pneumothorax in the image. Pneumothorax is a condition where the lung is collapsed and it can be seen in the chest X-ray image.\n",
    "\n",
    "![Normal vs Pneumothorax](https://assets.aboutkidshealth.ca/akhassets/Pneumothorax_XRAY_MEDIMG_PHO_EN.png?RenditionID=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we're given 100 images with equal number of Pneumothorax and non-Pneumothorax images. For our training purpose, we divide the dataset into training and validation sets (7:3) with same ratio as in original dataset.\n",
    "\n",
    "Thus, we'll have 70 images in training set and 30 images in validation set. Then we train our model on 70 images and we'd like to evaluate the model using validation set. Let's look at our Evaluation metrics for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy score\n",
    "\n",
    "Accuracy score is the ratio of correct predictions out of total in the target. Let's say in our above example, out of 30 images in validation set, the model predicts 27 images correctly. Then we can say model predicts with 90% accuracy or 0.9 accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_v1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: the accuracy score\n",
    "    \"\"\"\n",
    "    # Assign correct variable to zero, which will contain total correct predictions out of the actual values \n",
    "    correct = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct += 1\n",
    "    return 1.0 * correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the accuracy score for the sample data\n",
    "targets = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "preds   = [0, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "accuracy_score_v1(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that our `accuracy_score_v1` method gives the accuracy score of 0.8 or 80%.\n",
    "\n",
    "`sklearn` python package provides a function to calculate the accuracy score which we can use to cross check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from `sklearn` package's method we get same accuracy score which means our implementation of accuracy score is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider another example, where we are given a dataset of 100 images in which only 10 images has Pneumothorax and rest are non-Pneumothorax. If we divide the dataset into training and validation as 80:20 with equal ratio of images. Then training set will contain 72 images of non-Pneumothorax and 8 images of Pneumothorax. Similarly, validation set will contain 18 images of non-Pneumothorax and 2 images of Pneumothorax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, if we always predict non-Pneumothorax for any image, then still we'll get 90% accuracy without building a model. But would that be a good case? Definitely not. So we can see that having evaluation metric as `accuracy_score` for all problems wouldn't work. Specially, not in the cases where target variable is skewed. Here comes the `precision`, `recall`, `F1-score`, etc. for the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we move forward with Precision and others, we need to be familiar with some terminologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive or TP**: It is defined as if the model predicts `True` where the actual value is also `True`, then consider it as True Positive.  \n",
    "**True Negative or TN**: It is defined as if the model predicts `False` where the actual value is also `False`, then consider it as True Negative.  \n",
    "**False Positive or FP**: It is defined as if the model predicts `True` where the actual value is `False`, then consider it as False Positive.  \n",
    "**False Negative or FN**: It is defined as if the model predicts `False` where the actual value is `True`, then consider it as False Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of True Positive\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of true positive\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of True Negative\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of true negative\n",
    "    \"\"\"\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of False Positive\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of false positive\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of False Negative\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of false negative\n",
    "    \"\"\"\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:4\n",
      "TN:4\n",
      "FP:1\n",
      "FN:1\n"
     ]
    }
   ],
   "source": [
    "# Let's take same values above targets and preds.\n",
    "targets = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "preds   = [0, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "# We know that, TP = 4, TN = 4, FP = 1, FN = 1.\n",
    "# Let's see what our functions gives\n",
    "print(f'TP:{true_positive(targets, preds)}')\n",
    "print(f'TN:{true_negative(targets, preds)}')\n",
    "print(f'FP:{false_positive(targets, preds)}')\n",
    "print(f'FN:{false_negative(targets, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!! we have same values as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you might be thinking, couldn't we define Accuracy score using terms TP, TN, FP, FN? Yes we can and here's the definition:\n",
    "\n",
    "$ \\text{accuracy_score} = \\dfrac{TP + TN}{TP + TN + FP + FN} $\n",
    "\n",
    "We'll define our new function for `accuracy_score` using these terms internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_v2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: calculated accuracy score from the given values\n",
    "    \"\"\"\n",
    "    \n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    return (tp + tn) / (tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score (v1)     : 0.8\n",
      "Acc score (v2)     : 0.8\n",
      "Acc score (sklearn): 0.8\n"
     ]
    }
   ],
   "source": [
    "print(f'Acc score (v1)     : {accuracy_score_v1(targets, preds)}')\n",
    "print(f'Acc score (v2)     : {accuracy_score_v2(targets, preds)}')\n",
    "print(f'Acc score (sklearn): {accuracy_score(targets, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's deep dive into other Evaluation metrics for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precision vs Recall\" title=\"Precision vs Recall\" style=\"height:750px\" align=\"right\" />\n",
    "*Mathematically, Precision is defined as the fraction of relevant instances among all retrieved instances.*\n",
    "\n",
    "In simple terms, it can be defined as the ratio of correctly predicted positives out of total predicted positives. \n",
    "\n",
    "$ Precision = \\dfrac{TP}{TP+FP} $\n",
    "\n",
    "This metric and others as well are helpful in case of skewed target variable. Considering an example above, our accuracy was 90% though the Precision would be 0, because there are no True Positive found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the precision score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: Precision score for the given values\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    return 1.0*tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8\n",
      "Precision: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Consider another example to actually show power of Precision over Accuracy score\n",
    "targets = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "preds   = [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "print('Accuracy : {}'.format(accuracy_score_v2(targets, preds)))\n",
    "print('Precision: {}'.format(precision(targets, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our accuracy is 80% but our precision is just 50% and this conveys that our model is having issues with predicting Positives correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recall\n",
    "\n",
    "*Mathematically, it is defined as the fraction of retrieved instances among all relevant instances.*\n",
    "\n",
    "It can be defined as the ratio of correctly predicted positives out of all actual positives.\n",
    "\n",
    "$ Recall = \\dfrac{TP}{TP+FN} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the recall score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: Recall score for the given values\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    return 1.0*tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8\n",
      "Precision: 0.5\n",
      "Recall   : 0.5\n"
     ]
    }
   ],
   "source": [
    "# Consider another example to actually show power of Precision, Recall over Accuracy score\n",
    "targets = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "preds   = [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "print('Accuracy : {}'.format(accuracy_score_v2(targets, preds)))\n",
    "print('Precision: {}'.format(precision(targets, preds)))\n",
    "print('Recall   : {}'.format(recall(targets, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a way to calculate Precision and Recall scores. We know that these both can range from 0 to 1. Both metrics having value closer to 1 suggests that we have a good model. But having look at both the metrics can be confusing when either of them is high and another is low. To rescue this, we have **F1-Score**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "\n",
    "It is another metric which helps us in evaluating the model in case of skewed target variable. It is the simple harmonic mean of Precision and Recall.\n",
    "\n",
    "$ \\text{f1_score} = \\dfrac{2PR}{P+R} $, where, `P` = Precision and `R` = Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the F1 score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: F1 score for the given values\n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return (2*p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8\n",
      "Precision: 0.5\n",
      "Recall   : 0.5\n",
      "F1 score : 0.5\n"
     ]
    }
   ],
   "source": [
    "# Consider another example to actually show power of Precision, Recall over Accuracy score\n",
    "targets = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "preds   = [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "print('Accuracy : {}'.format(accuracy_score_v2(targets, preds)))\n",
    "print('Precision: {}'.format(precision(targets, preds)))\n",
    "print('Recall   : {}'.format(recall(targets, preds)))\n",
    "print('F1 score : {}'.format(f1_score(targets, preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have other terms like `TPR` and `FPR` which helps us in defining other important metrics further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate (TPR)\n",
    "\n",
    "It is same as Recall. This is also known as **Sensitivity**.\n",
    "\n",
    "$ TPR = \\dfrac{TP}{TP+FN} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the True Positive Rate (TPR)\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: TPR for the given values\n",
    "    \"\"\"\n",
    "    return recall(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate (FPR)\n",
    "\n",
    "It is defined as,\n",
    "\n",
    "$ FPR = \\dfrac{FP}{TN+FP} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the False Positive Rate (FPR)\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: FPR for the given values\n",
    "    \"\"\"\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    return 1.0 * fp / (tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate,\n",
    "\n",
    "$ 1 - FPR $, that gives us **True Negative Rate (TNR)**, also known as **Specificity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in case of any classification model, the model predicts probabilities for each class and then we based on a threshold value, calculates the class for which prediction belongs. For an example, if a binary classifier model predicts probability that 0.8 for Class 1 and 0.2 for Class 2. Based on the threshold (say, `0.5` most of the times), we calculate the class to be *Class 1* (since, 0.8 > 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Area under ROC (Receiver Operating characteristics) or AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, interesting thing to note above is that our TPR or FPR depends on the threshold values we choose. And that's where it gets trickier to predict class and which threshold to choose that works better in the context of the problem.\n",
    "\n",
    "For now, let's plot TPR and FPR for all threshold values in between 0 and 1 for the sake of our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for TPR and FPR for each threshold value\n",
      "   Threshold  TPR  FPR\n",
      "0       0.00  1.0  1.0\n",
      "1       0.10  1.0  0.9\n",
      "2       0.20  1.0  0.7\n",
      "3       0.30  0.8  0.6\n",
      "4       0.40  0.8  0.3\n",
      "5       0.50  0.8  0.3\n",
      "6       0.60  0.8  0.2\n",
      "7       0.70  0.6  0.1\n",
      "8       0.80  0.6  0.1\n",
      "9       0.85  0.4  0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcYAAAG+CAYAAAAJE6SFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcdZ3v8fe3u7PvO0tCEiCQxB0juLMFB7kO3HvH68jVmcFxuTrX2VzuOHdGREefmdERBEEBRy+4IIvbZBQHCauALFGEYSeEkAQIISErIfvv/nFOd34pe6Pp1Knqfr+ep5+us9Spb/3q1PnU75xTpyKlhCRJKrRUXYAkSY3EYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqO6FBEpIp6PiC9UWMMhEbElIlqrqqHRRMTPI+JPKnrs5RGxsIrH7m8RcWFEfLoP9+vzOhkRUyLioYgY8WLv+1JExA8j4u31fMy+iIjPltucFBFtVdUx6IOxXMHb//ZExAvZ8Hsi4qyI2FkOb4iI2yLiDeV9z4iI3eW0TRFxT0S8ow41L6+pc0tEHBQRs8oVqn3c8oj4VHa/9qDbEhFPRsTZvXhzvyql9Hfl/btd/v6QUlqRUhqdUtq9Px+nXURMj4jvRcS6sq3urMdr2k09Z0XEd/NxKaW3p5Qu3U+PNzYivhIRK8rX+LFyePL+eLy+iogbI+IDL2UZKaUPp5T+oRePtc+HgZe4Tn4KuCSl9EK57BsjYlvZ1msj4kcRcWDN48+PiEURsTEiNkfEDRHxxpp5hpbryqPlers8Ir4VEbPKWf4Z+Hwf6q2rlNJngJdVXcegD8ZyBR+dUhoNrAB+Pxv3vXK2K8rpU4BbgB9FRJTTflVOGw98Dbg8IsbXofS8ztEppaeyaePLmk4HzoyIk7NpryqnHQv8IfCnfXjs9uW/E/h0RJzU1ydRlc4+jUbERIrXdwfFm3MycA5wWUS8sx41VCkihgLXUTz3k4GxwBuAdcDR/fxYERGVbX+q2AMREcOAPwG+WzPpo+X76XBgNPAv2X0OA24F/hOYDRwE/Bj4RfsH9NIPgFOB/wmMA14F/Bo4ESCldCcwNiIW9P8z26vR1uk+Syn5V/4By4GFNePOAr6bDb8MSBQbzTOAW7JpI8tpr+ti+Q8C78iG24BngaOA4RRvmHXABuAuYFpv6yzHzyofvy0bdxfwifJ2Ag7Ppl0JXNBNe9TO39ny7wQ+mQ3/afk81wPXADNr2u5a4DngGeD/luNbKD5JP1Y+/yuBibWPSRHkS2pq/GtgUXl7GMVGZUW5/AuBEeW044BVwN8Aq4HvdPJ8/wG4D2ipGf83wBNAZO3yF8AyYC3wpfw+PbRBAv438CjweDnuXGAlsIliY/aWcvzJFCG9E9gC3FOOvxH4QHn7DIow/5fy8R4H3p493mzgZmAzsBi4gGx9rnmeHyjbbXQP75FPAPcCG4ErgOHltAnATynW6fXl7enZfW8EvkCxoX+BIgjeV7bV5rI9/1fN450G/LZsm8fKNvkCsBvYVrbL+eW8c9m7fj0MvCtbziXA14GrgeeBheW4z5fTJ5f1bijv/0uK9fI7wJ6y3i3A/6HmfQBMBP4f8FT5vH/SRdu9FVhaM67jtSyH/wy4Pxv+DnB1J8v6OnBzeXthWd+MHrZv3wA+083032nrzrY3ZNvErC3eT/G+uxn4OUXY58u+B/jvPb1OXW1n6v1XyYM26l/tCtDJSjCMYiO4ohw+gzIYgVaKDd4OYGoXyz8T+F42/F+AB8vb/wv4d4pwbQVeC4ztbZ21KxQQwJuArcCJ5fSOoCtXzqeBv+6mPboNRuD15fL/Wzl8GrAUmFfW8PfAbeW0MeXjfZziQ8AY4Jhy2l8CtwPTyza+CPh+J89pJMUGdE5W013Au8vb5wCLKDZUY8r2/Mdy2nHALopdSsMoA7Pm+d4OfLaT8bPLGo7M2uWG8nEOAR5hb1B12QbZfa8t79se2u8FJpXzf5wiuNvD5ixqgozfDcadwAfL9eYjFBvo9hD/FUVoDgXeTLHR6yoYLwcu7cV75E6KnstEilD7cDltEvAH5es0BriKLCTKuldQfEBqA4ZQvAcOo1hfj6VYn44q5z+aInxPogipg4G5tW1QDo+i+HDxvnLZr6H40DK/nH5Juaw3lcsazr7B+I8UH6SGlH9vydpwOfsGwyz2fR/8jOIDwoTyvsd20Xb/G/hZN6/lJIoPL/+WTV8NvK+TZR1P8eFgBPBPwE292L59DPhRF9O6a+va538WvxuM3y5fgxHAHwO3ZvPPp/jAMayn16mz9q3ir/IwaqS/2hUgWwl2lC/sGuB64LXltDMoNrYbKDZOL1Dz6admWYdTbNhHlsPfA84sb/8pcBvwyl7WuaV83A2UG59shdpA8cn1QeAvsvslig3j8+Xt7wPDunmcroJxQ/lcE8VGt30D8nPg/dn8LRQbupkUu3Xv7uJxHqQM73L4wLI922rfJBS96vY2m9PenhQb1ueBw7LlvIG9vbLjytdxeDfPdynlRr5m/PCyhjdl7XJyNv3PgOt6aoPsvif08Pqup9jl3b7+9RSMS7Np7XstDqAI7V3t61vWfl0F47XAP/Vi3XtvNvxF4MIu5n01sL6m7s/1sPyfAH9Z3r4IOKeL+TraoBz+Q+CXNfNcRNlDogjBb9dMv4S9wfg54N/I1vea59xpMJbr6h5gQi/et38HXN7J89hKEUqJosd2SDZ9V76uZePnlvMfTNETvLwXj/9B4PoupnXX1rXPv2OdzNri0Gz6GIr3Yvs6/wXgW715nWrbt6fntL/+Bv0xxl66MqU0PqU0NaV0Qkrp19m021NK4yk+LS6i+KTZqZTSUooQ+P2IGElxTOCycvJ3KHa7XR4RT0XEFyNiSDc1/deypvEppf9aM21ySmlCSmleSum8mmlHURzH+EPgGIpPcC/W5HIZH6cInPY6ZwLnlicpte+SCoo37wyK3TOdmQn8OLvfgxSfhqd1Mu9lFCELxfGUn6SUtlIc/x0J/Dpbzn+U49s9m1La1s3zWkuxoat1YDa93crs9hMUPaj259JVG3R2XyLiExHxYHlyxQaKY0Qv5mSX1e03yraA4vU5CHguG/c7j11jHZ0//y4fj2KjPhogIkZGxEUR8UREbKLYrTa+5nhe7XN/e0TcHhHPlc/9FPY+9+7WmVozgWPa271c1nsoPiB0+tg1vkTxwegXEbHsRZxUNoOijdf3Yt71FKFR6y9SSuOAV1JsR6Zn07pbJ/eUy+zt6zaG4kNtZ15MW3emo21TSpspetHvLkedTtEJgN69TpUzGPtJSmkLxW6sP4qI13Qz6/cpVpTTgAfKsCSltDOl9NmU0nzgjcA7KHZJ7I9aU0rpSordbGf2cRm7U0pnUxzn+bNy9EqKY0Tjs78RKaXbymmHdrG4lRTHxfL7DU8pPdnJvNcCUyLi1RTt2P7BYi1FL/Zl2TLGpeKkho6ye3hai4H/3slJIe8qa3wkGzcju30Ixe7L9ufSVRv8Th0R8RaK41bvouh1jKfoPUTtvH3wNDCx/BDWWd21FgO/FxF9+bAExQelIyl2kY+lOKYGe58L7PvchwE/pNjrMK187ldn86+k2M3amdp2WUmxOzFv99EppY90c5+9E1LanFL6eErpUIoPrB+LiBN7ul/5uBN7ecLdvcAR3dTwnxRnjl6Qndy3GPgfncz+LooT/7aW8xwdEdM7mS83j+JYX2e6a+vnKT50tussxGrb6PvA6eUJQsMpDj20P05Pr1PlDMZ+lFJ6DvhXug+by4G3UYRo+0adiDg+Il5RfrreRLErcc9+LBeKYxMfjIiX8mntn4D/ExHDKY7R/G1EvAwgIsZFRPub+qfAgRHxVxExLCLGRMQx5bQLgS9ExMzyflMi4rTOHiyltJPi2NWXKI5xXVuO30OxS+mciJhaLufgiPi9F/FczqHorX0zIg6IiOERcTrFLrBPpnI/T+mTETEhImZQHCO9InsuXbVBZ8ZQ7C57FmiLiDMpzgZt9wwwqy9ncKaUngCWAGeVp/O/Afj9bu7yHYoN1w8jYm5EtETEpIj4vxFxSi8ecgzFh5MN5Rm+n+lh/qEUx52eBXZF8T27t2XTvwm8LyJOLGs5OCLmltOeYd8PWj8FjoiIP4qIIeXf6yJiXi/qJiLeERGHl4G0kWKPRfv7r/axOqSUnqbYff61cn0YEhFv7WxeimOz4yPi4C6mA1xKsafk1HL4s8AbI+ILETGxfN/8OcWH5r8pa1hM8T74cUS8NiLayvk+HBH5WefHlrV2pru2/i3w7vK5LaA4G70nV1P0Dj9HcVZ/e1u+pNepXgzG/vcV4JSIeGVnE8s30q8oeoVXZJMOoDjlehPFrsSbKDZU+035CfVm4JMvYTE/o9id88GU0o8pTm65vNyVdh/w9vKxNlMc2P99il1xj1KcQADFWZmLKHZjbaY4CeYYunYZxZl4V6WUdmXj/4Zid9jt5eMvpujB9EpKaR3FCSrDgQcodlF9DPijlNIVNbP/G8UZpL8t2+Cb5TK6bIMuXEOxy/cRil2y29h3l99V5f91EfGb3j6XzHvY+5WLz1Osc9s7mzGltJ2iXR+i2NBuotiYTwbu6MVjfYXi5Iu1FK/hf3Q3c7lO/AXFWcjrKXaNL8qm30lxksY5FGF1E8XGFop15p0RsT4iziuX9TaK3XdPUaxj7Sda9cYcivVlC8X782sppfZezj8Cf1/u+vtEJ/f9I4oPsg9RnIfwV1083x0UxzXf21UR5TznAp8uhx+lWCdfRXGs72mKE5x+L6V0a3bXd1KE0RUUbXUfsKB8TkTE64AtZZt29rjdtfWnKXqT6ymC+rLOllGzvO3AjyjWp8uy8S/1daqL2PdDsLRXRGyj2Iiel1J60VcIGagiIlGcGbu06lperIi4AngoFV+kVp1FxBSKr4K8JpVf8q/T4/4Q+GZK6ep6PWZfRMRnKD6MDgNGpTpd2ON36jAYpRenmYKx7Ck8R/H9xrdRnPX5hpTS3ZUWJjWwuu5KjeISRWsi4r4upkdEnBcRSyPi3og4qp71SQPQARRfCdgCnAd8xFCUulfXHmN5UHoLxfeJXt7J9FOAP6c4ZfsY4NyUUnfHmiRJ6ld17TGmlG6m2K3TldMoQjOllG6nOIOrN9/PkSSpXzTaBV8PZt8z8laV456unTEiPgR8CGDUqFGvnTt3bu0sktStTdt28sS6rT3PqKa0Y/XStSmlKT3Pua9GC8ZeSyldDFwMsGDBgrRkyZKKK5LUTPbsSZxy3i/ZsXozABNHDWXUUH/2c6AY2tbC9Z84/om+3LfRgvFJ9r0yx/RynCT1q5/ft5qHylAc0hp8+NjDGD2s0TaJ6qsZE0dwfWffOu2FRvuC/yLgj8uzU18PbCy/EC9J/Wb3nsQ5i/de4e+Nh002FNWhrmtCRHyf4qLTkyNiFcUlo4YApJQupLhywykUVy/ZSnElBknqV4vueZKla7YAMKythbcc/mKu2a6Brq7BmFI6vYfpieI3yyRpv9i5ew/nLn60Y/hNh09mpL1FZRptV6ok7Vc/+s0qlpdnoo4Y0sqb7S2qhsEoadDYvms3512390p+b5kzmeFDPBNV+zIYJQ0aV961kic3FNfuHjW0lTccNqniitSIDEZJg8K2nbs5/4a9vcW3HjGFYW32FvW7DEZJg8L37ljBM5uKn6IcM7yNY2bbW1TnDEZJA97WHbv4+o17e4vHHTGFoW1u/tQ51wxJA96ltz3B2i07ABg3YgivmzWx4orUyAxGSQPa5m07uejmxzqGTzhyKm2tbvrUNdcOSQPat25ZzoatO4HiQuFHzZxQcUVqdAajpAFrw9Yd/Osvl3UMnzB3Kq0tUWFFagYGo6QB6xu/XMbm7bsAmDx6GK+eMb7iitQMDEZJA9K6Ldv5f7cu7xg+cd5UWsLeonpmMEoakC686TG27tgNwLSxw3jFweMqrkjNwmCUNOCs2bSNb/9q74+3L5w3zd6ies1glDTgfO3Gx9i+aw8AB40fzvwDx1ZckZqJwShpQHlywwtcdseKjuGT5k0j7C3qRTAYJQ0o51+/lB27i97ijAkjOGLamIorUrMxGCUNGCvWbeWqJSs7hk+af4C9Rb1oBqOkAePc6x5l154EwOzJozhsyqiKK1IzMhglDQiPPbuFH9+9qmN4occW1UcGo6QB4SuLH6XsLDJn6mhmT7a3qL4xGCU1vYdXb+an9z7VMbxw3rQKq1GzMxglNb1zrn2EVPYW5x4whhkTR1ZbkJqawSipqd335Eb+4/7VHcP2FvVSGYySmtrZ1z7ScftlB43loPEjKqxGA4HBKKlp/fqJ9Vz/0BoAAnuL6h8Go6SmdU7WW3zl9HFMGzu8wmo0UBiMkprS7cvWccvStQC0BJw4196i+ofBKKnppJQ4+xd7e4uvmTGByWOGVViRBhKDUVLTuWXpWu5c/hxQ9BaPnzu14oo0kBiMkppKSol/yXqLC2ZNZOKooRVWpIHGYJTUVK5/aA33rNwAQFtLcPyR9hbVvwxGSU1jz560z/cWj549kXEjhlRYkQYig1FS07jm/tXc/9QmAIa0BsceMaXiijQQGYySmsLuPYlzFu/tLb7+0EmMGW5vUf3PYJTUFH5671M88swWAIa2tfDWOfYWtX8YjJIa3q7de/jK4kc7ht902CRGDWursCINZAajpIb347uf5PG1zwMwfEgLbz7c3qL2H4NRUkPbsWsP5163t7f45sOnMGJoa4UVaaAzGCU1tKt+vZJV618AYOTQVt502KSKK9JAZzBKaljbdu7mq9ct7Rh+65wpDBtib1H7l8EoqWF9/84VrN60DYDRw9p4/aH2FrX/GYySGtILO3ZzwQ2PdQwfe8QUhra5ydL+51omqSF9+1fLWbtlOwBjh7dx9OyJ1RakQcNglNRwtmzfxYU37e0tHj93KkNa3VypPlzTJDWcS259nPVbdwIwYeQQXjtzQsUVaTAxGCU1lI0v7OTim5d1DJ8wdyptLW6qVD+ubZIayjd/uYxN23YBMGnUUF49w96i6stglNQwnnt+B9+85fGO4RPnTaO1JSqsSIORwSipYVx082M8v2M3AFPHDOOV08dVXJEGI4NRUkNYs3kbl962vGN44bxptIS9RdWfwSipIXz9xsfYtnMPAAeOG878g8ZWXJEGK4NRUuWe3vgC37tjRcewvUVVyWCUVLnzr1/Kjl1Fb3H6hBHMPWBMxRVpMDMYJVVq5XNbuXLJyo7hk+ZNI+wtqkIGo6RKffX6R9m5OwEwc9JIDp86uuKKNNgZjJIq8/ja5/nhb57sGD5pvr1FVc9glFSZcxc/wu49RW/xsCmjOHSyvUVVz2CUVIlHntnMv93zVMfwSfOmVViNtJfBKKkSX1n8CKnoLHLktDEcMmlUtQVJJYNRUt3d/9RGrv7P1R3DC+0tqoEYjJLq7pxrH+m4Pf/AsRw8YUSF1Uj7Mhgl1dVvV25g8YNrAAjsLarxGIyS6urLv3i44/Yrpo/jgHHDK6xG+l0Go6S6uWv5c/zy0bVA0Vs8ca69RTUeg1FSXaSU+Jdr9vYWXz1jPFPGDKuwIqlzBqOkurjtsXXc8fhzALQEnDB3asUVSZ2rezBGxMkR8XBELI2IT3Uy/ZCIuCEi7o6IeyPilHrXKKl/pZT2Obb42pkTmDTa3qIaU1s9HywiWoELgJOAVcBdEbEopfRANtvfA1emlL4eEfOBq4FZ9axzMNizJ7F60zZS1YVoUPj1E+v5zYoNALS2BMcfaW9RjauuwQgcDSxNKS0DiIjLgdOAPBgT0P7T3eOAp1C/eu75Hbzz67exbO3zVZeiQeh1syYyfuTQqsuQulTvXakHAyuz4VXluNxZwHsjYhVFb/HPO1tQRHwoIpZExJJnn312f9Q6YF1ww1JDUZVoawmOO3JK1WVI3ap3j7E3TgcuSSl9OSLeAHwnIl6eUtqTz5RSuhi4GGDBggXuEeyl1Ru38d3bn+gYHju8zZ/5UV0MbW3h+LlTGDt8SNWlSN2qdzA+CczIhqeX43LvB04GSCn9KiKGA5OBNXWpcIC74IalbN9VfMaYPmEEHzn2MINRkjL13pV6FzAnImZHxFDg3cCimnlWACcCRMQ8YDjgvtJ+sGr9Vi6/a0XH8MJ5/iisJNWqazCmlHYBHwWuAR6kOPv0/oj4XEScWs72ceCDEXEP8H3gjJSSu0r7wVevW8rO3UVTzpw4kjlT/VFYSapV92OMKaWrKU6qycedmd1+AHhTvesa6JavfZ4f/GZVx/DC+fYWJakzXvlmkDjvukfZvafoLR46ZRSHTbG3KEmdMRgHgaVrNvPj3+49x+kkf+ZHkrpkMA4C5yx+lPajtEdMG83MSaOqLUiSGpjBOMA9+PQmfnbv0x3D/iisJHXPYBzgzr72kY7b8w4cy/QJIyusRpIan8E4gN27agPXPvBMx/DCeV64WZJ6YjAOYF/+xd7e4isOHseB40ZUWI0kNQeDcYD69RPPcdMjxQWDAjjRH4WVpF4xGAeovLf46hnjmTp2eIXVSFLzMBgHoNseW8ttj60DoCXgBHuLktRrBuMAk1Li7Ky3eNQhE5g0eliFFUlSczEYB5ibHnmWJU+sB6A1guPtLUrSi2IwDiAppX2+t7hg1gQmjBxaYUWS1HwMxgFk8YNruHfVRgDaWoLjj7S3KEkvlsE4QOzZk/jyLx7uGD5m9kTGjhhSYUWS1JwMxgHi5/et5qHVmwEY0hoca29RkvrEYBwAdu9JnLN477HFNx42mdHD6v4b1JI0IBiMA8Cie55k6ZotAAxra+Eth0+uuCJJal4GY5PbuXsP5y5+tGP4TYdPZqS9RUnqM4Oxyf3oN6tYvm4rACOGtPJme4uS9JIYjE1sx649nHfd0o7ht8yZzPAhrRVWJEnNz2BsYlcsWcmTG14AYOTQVt5w2KSKK5Kk5mcwNqltO3dz/vV7jy0ee8QUhrXZW5Skl8pgbFLfu2MFz2zaDsCYYW0cM9veoiT1B4OxCW3dsYuv37j32OJxR05haJsvpST1B7emTejS255g7ZYdAIwbMYTXzZpYcUWSNHAYjE1m87adXHTzYx3DJxw5lbZWX0ZJ6i9uUZvMt25ZzoatOwGYOGooR82cUHFFkjSwGIxNZOPWnfzrLcs6hk84ciqtLVFhRZI08BiMTeQbv1zG5m27AJg8ehivmjG+4ookaeAxGJvEui3b+datj3cMnzjP3qIk7Q8GY5O46OZlbN2xG4BpY4fxioPHVVyRJA1MBmMTWLNpG5fetrxjeOG8abSEvUVJ2h8MxibwtRsfY/uuPQAcNH448w8cW3FFkjRwGYwN7qkNL3DZHSs6hk+aN42wtyhJ+43B2OC+ev1SduwueoszJozgiGljKq5IkgY2g7GBrVi3lauWrOwYPmn+AfYWJWk/Mxgb2LnXPcquPQmA2ZNHcdiUURVXJEkDn8HYoB57dgs/vntVx/BCjy1KUl0YjA3q3MWPUnYWOXzqaGZPtrcoSfVgMDagh1dv5t/vfapj+KR50yqsRpIGF4OxAZ1z7SOksrc494AxzJg4stqCJGkQMRgbzH1PbuQ/7l/dMbzQ3qIk1ZXB2GDOvvaRjtsvO2gsB40fUWE1kjT4GIwN5Dcr1nP9Q2sACOwtSlIVDMYGcvYv9vYWXzl9HNPGDq+wGkkanAzGBnHHsnXcsnQtUPQWT5xrb1GSqmAwNoCUEl/OeouvOWQCk8cMq7AiSRq8DMYGcMvStdy5/DkAWgJOmDu14ookafAyGCtW21tcMHMiE0cNrbAiSRrcDMaK3fDwGn67cgMAbS3B8fYWJalSBmOFanuLr5s9kXEjhlRYkSTJYKzQNfev5v6nNgEwpDU47ogpFVckSTIYK7J7T9rnKjevP3QSY4bbW5SkqhmMFfnpvU/xyDNbABja1sJb59hblKRGYDBWYNfuPZy7+NGO4TceNolRw9oqrEiS1M5grMCP736SZWufB2D4kBbecri9RUlqFAZjne3YtYfzrt/bW3zz4ZMZMbS1wookSTmDsc6u+vVKVj73AgAjh7byxsMmV1yRJClnMNbRtp27Of/6pR3Db50zheFD7C1KUiMxGOvo8jtX8PTGbQCMGtbG6w+dVHFFkqRaBmOdvLBjN+ff8FjH8HFHTGFom80vSY3GLXOdfOf25azdsh2AscPbOHr2xIorkiR1xmCsgy3bd3HhTcs6ho87cipDWm16SWpEbp3r4JJbH+e553cAMH7kEBbMmlBxRZKkrhiM+9nGF3Zy8c17e4snHDmVthabXZIalVvo/eybv1zGpm27AJg0aiivOcTeoiQ1sroHY0ScHBEPR8TSiPhUF/O8KyIeiIj7I+KyetfYX9Y/v4Nv3bq8Y/jEeVNpbYnqCpIk9aiuV66OiFbgAuAkYBVwV0QsSik9kM0zB/hb4E0ppfUR0bQ/aX/RzcvYsr3oLU4dM4xXTh9fcUWSpJ7Uu8d4NLA0pbQspbQDuBw4rWaeDwIXpJTWA6SU1tS5xn6xdst2Lr1tecfwifOm0RL2FiWp0dU7GA8GVmbDq8pxuSOAIyLi1oi4PSJO7mxBEfGhiFgSEUueffbZ/VRu392+bB0v7NwNwAFjh/Oyg8ZWXJEkqTca8eSbNmAOcBxwOvCNiPidfZAppYtTSgtSSgumTGm8n23asWtPx+0Dxg23tyhJTaLewfgkMCMbnl6Oy60CFqWUdqaUHgceoQhKSZL2u3oH413AnIiYHRFDgXcDi2rm+QlFb5GImEyxa3UZkiTVQV2DMaW0C/gocA3wIHBlSun+iPhcRJxaznYNsC4iHgBuAD6ZUlpXzzolSYNXXb+uAZBSuhq4umbcmdntBHys/JMkqa4a8eQbSZIqYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRl+iUYI2JEfyxHkqSqvaRgjIjJEfFZYEU/1SNJUqXaupsYEW8G3gvMAJYB56WUHo2IA4BPA2eUy/jufq5TkqS66DIYI+I04LV+RjIAAA1NSURBVEfAemAp8CrgPRFxBnAJMBL4V+CLKSV7jJKkAaG7HuPfAj8H/kdK6YWICOCLwA+Bh4FTU0rL6lCjJEl1090xxrnA+SmlFwBSSokiGFuBvzcUJUkDUXfBOBZ4rmZc+7C7TiVJA1K3J98AsyNiSzbcWv4/NCK25TOmlB7o18okSapAT8F4WRfjrwRSeTvK261dzCtJUtPoLhiPr1sVkiQ1iC6DMaV0Uz0LkSSpEfT0Bf9XAh8EZgGrgR+llH5eh7okSapEl2elRsQJwBLgPcAU4BTgpxHx8TrVJklS3XX3dY3PAjcBM1JKr6e4LNz5wGciwl/lkCQNSN0F3MuAs1NKzwOklPYAXwBGAzPrUJskSXXXXTCOB9bVjGsfnrB/ypEkqVp+wV+SpIxf8JckKeMX/CVJynQXjAn4TUppSzfzSJI0oHR38s0NwPx6FSJJUiPoLhijblVIktQg/KK+JEmZns5KPSUi5vZmQSmlb/dDPZIkVaqnYDyzl8tJgMEoSWp6PQXj8RQXEpckaVDoKRhfaL9WqiRJg4En30iSlDEYJUnKdLkrNaVkaEqSBh3DT5KkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVKm7sEYESdHxMMRsTQiPtXNfH8QESkiFtSzPknS4NZWzweLiFbgAuAkYBVwV0QsSik9UDPfGOAvgTt6s9wHn97Egs8v7u9yX5LtO3dXXYIkqQ/qGozA0cDSlNIygIi4HDgNeKBmvn8A/hn4ZG8WumtPYu2W7f1ZZ79qa4mqS5Ak9VK9d6UeDKzMhleV4zpExFHAjJTSz7pbUER8KCKWRMSS/i+z/4wa2sprZ06ougxJUi/Vu8fYrYhoAc4Gzuhp3pTSxcDFAMMOnJNee8gE3vayafu3wD4YMbSVthbPcZKkZlHvYHwSmJENTy/HtRsDvBy4MSIADgAWRcSpKaVue4ZD2loYM3xIP5crSRps6t2VuQuYExGzI2Io8G5gUfvElNLGlNLklNKslNIs4Hagx1CUJKm/1DUYU0q7gI8C1wAPAlemlO6PiM9FxKn1rEWSpM7U/RhjSulq4OqacWd2Me9x9ahJkqR2nhUiSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUMRglScoYjJIkZQxGSZIyBqMkSRmDUZKkjMEoSVLGYJQkKWMwSpKUqXswRsTJEfFwRCyNiE91Mv1jEfFARNwbEddFxMx61yhJGrzqGowR0QpcALwdmA+cHhHza2a7G1iQUnol8APgi/WsUZI0uNW7x3g0sDSltCyltAO4HDgtnyGldENKaWs5eDswvc41SpIGsXoH48HAymx4VTmuK+8Hft7ZhIj4UEQsiYgl/VifJGmQa9iTbyLivcAC4EudTU8pXZxSWpBSWlDfyiRJA1lbnR/vSWBGNjy9HLePiFgI/B1wbEppe51qkySp7j3Gu4A5ETE7IoYC7wYW5TNExGuAi4BTU0pr6lyfJGmQq2swppR2AR8FrgEeBK5MKd0fEZ+LiFPL2b4EjAauiojfRsSiLhYnSVK/q/euVFJKVwNX14w7M7u9sN41SZLUrmFPvpEkqQoGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJGYNRkqRM3YMxIk6OiIcjYmlEfKqT6cMi4opy+h0RMaveNUqSBq+6BmNEtAIXAG8H5gOnR8T8mtneD6xPKR0OnAP8cz1rlCQNbm11fryjgaUppWUAEXE5cBrwQDbPacBZ5e0fAOdHRKSUUncLHj6khYmjhvR/xZKkpjN6WN/jrd7BeDCwMhteBRzT1TwppV0RsRGYBKzNZ4qIDwEfKge3X/qnx9x36X4peUCbTE27qldst76x3frOtuubI/typ3oHY79JKV0MXAwQEUtSSgsqLqnp2G59Y7v1je3Wd7Zd30TEkr7cr94n3zwJzMiGp5fjOp0nItqAccC6ulQnSRr06h2MdwFzImJ2RAwF3g0sqplnEfAn5e13Atf3dHxRkqT+UtddqeUxw48C1wCtwLdSSvdHxOeAJSmlRcA3ge9ExFLgOYrw7MnF+63ogc126xvbrW9st76z7fqmT+0WdsYkSdrLK99IkpQxGCVJyjRVMHo5ub7pRbt9LCIeiIh7I+K6iJhZRZ2Npqd2y+b7g4hIEeHp9PSu3SLiXeU6d39EXFbvGhtRL96nh0TEDRFxd/lePaWKOhtNRHwrItZExH1dTI+IOK9s13sj4qgeF5pSaoo/ipN1HgMOBYYC9wDza+b5M+DC8va7gSuqrrvqv1622/HAyPL2R2y33rVbOd8Y4GbgdmBB1XVX/dfL9W0OcDcwoRyeWnXdVf/1st0uBj5S3p4PLK+67kb4A94KHAXc18X0U4CfAwG8Hrijp2U2U4+x43JyKaUdQPvl5HKnAe0XwPkBcGJERB1rbEQ9tltK6YaU0tZy8HaK75cOdr1Z3wD+geJ6vtvqWVwD6027fRC4IKW0HiCltKbONTai3rRbAsaWt8cBT9WxvoaVUrqZ4hsMXTkN+HYq3A6Mj4gDu1tmMwVjZ5eTO7ireVJKu4D2y8kNZr1pt9z7KT5dDXY9tlu5S2ZGSuln9SyswfVmfTsCOCIibo2I2yPi5LpV17h6025nAe+NiFXA1cCf16e0pvdit4HNe0k49b+IeC+wADi26loaXUS0AGcDZ1RcSjNqo9idehzF3ombI+IVKaUNlVbV+E4HLkkpfTki3kDxfe+Xp5T2VF3YQNNMPUYvJ9c3vWk3ImIh8HfAqSml7XWqrZH11G5jgJcDN0bEcopjF4s8AadX69sqYFFKaWdK6XHgEYqgHMx6027vB64ESCn9ChhOcXFxda9X28BcMwWjl5Prmx7bLSJeA1xEEYoe7yl0224ppY0ppckppVkppVkUx2ZPTSn16aLFA0hv3qc/oegtEhGTKXatLqtnkQ2oN+22AjgRICLmUQTjs3WtsjktAv64PDv19cDGlNLT3d2haXalpv13ObkBrZft9iVgNHBVea7SipTSqZUV3QB62W6q0ct2uwZ4W0Q8AOwGPplSGtR7dnrZbh8HvhERf01xIs4ZfvCHiPg+xQetyeXx188AQwBSShdSHI89BVgKbAXe1+MybVdJkvZqpl2pkiTtdwajJEkZg1GSpIzBKElSxmCUJCljMEoNLCLOKn+5o/ZvcTl9eTZuR0Q8FBGfLr8LRxfzPBoR/xwRo6p7ZlLjaprvMUqD2Eag9nqiG7PblwFfBYZR/FLKZyiu+vSJTuYZSnHJv09TXEf4A/unZKl5GYxS49tV/ipAV57Opt8UEdOBD0fEJ7MvgOfz3BwRBwN/EhEf8lqb0r7clSoNPL8GRtH9dTTvobik2JS6VCQ1EXuMUhMoL4qf293N5cBmATvo/jfqDgE2A2tfenXSwGKPUWp8k4CdNX8nZtMjItoiYmREvAP4MPDvKaXdXcxzcjnPF2rmkYTXSpUaWkScBfwVsLBm0sMppc3lT17NrJn2U+ADKaVnymV0Ns+PUkp/0O8FSwOAu1Klxrerh5+z+i5wLrAdWJ5S2tzNPKMofprtfRHxkZTS1/u9WqnJGYxS83umF78Dmc9zU0TMBD4XEd9OKT2/n+uTmorHGKXB6W8pzlp9f9WFSI3GYJQGoZTSncC1wF9HRGvV9UiNxGCUBq/PU3y1410V1yE1FM9KlSQpY49RkqSMwShJUsZglCQpYzBKkpQxGCVJyhiMkiRlDEZJkjIGoyRJmf8Prc3BXSLhZhMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tprs = [] # Empty list to contain values of TPR for corresponding threshold value\n",
    "fprs = [] # Empty list to contain values of FPR for corresponding threshold value\n",
    "\n",
    "#actual target values\n",
    "y_true = [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
    "\n",
    "# predicted probabilities of a sample being 1\n",
    "y_pred_probs = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]\n",
    "\n",
    "# handmade thresholds\n",
    "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n",
    "\n",
    "# Book keeping in DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    y_pred = [1 if prob >= threshold else 0 for prob in y_pred_probs]\n",
    "    \n",
    "    tpr_ = tpr(y_true, y_pred)\n",
    "    fpr_ = fpr(y_true, y_pred)\n",
    "    \n",
    "    tprs.append(tpr_)\n",
    "    fprs.append(fpr_)\n",
    "    \n",
    "    temp = pd.DataFrame({'Threshold': threshold,'TPR': tpr_, 'FPR': fpr_}, index=[i])\n",
    "    df = df.append(temp)\n",
    "\n",
    "print('Data for TPR and FPR for each threshold value')\n",
    "print(df.head(10))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.fill_between(fprs, tprs, alpha=0.4)\n",
    "plt.plot(fprs, tprs, lw=3)\n",
    "plt.xlim(0, 1.0)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('FPR', fontsize=15)\n",
    "plt.ylabel('TPR', fontsize=15)\n",
    "plt.title('TPR vs FPR [Receiver Operating Characteristic (ROC) curve]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above curve gives us **Receiver Operating Characteristic Curve (ROC curve)** and the **Area under ROC curve** or commonly known as **AUC** is another metric for evaluation.\n",
    "\n",
    "We can use `sklearn` package to get area under above ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8300000000000001"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_true, y_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand AUC values which are in range of 0 to 1.\n",
    "\n",
    "+ **AUC = 1**: It implies that the model is a perfect model, ideally. But in real world, it indicates that we have made some mistake in our data processing or validation step and we need to go back and check the process.\n",
    "+ **AUC = 0**: It implies that the model is worse (Or perfect). Normally, we should try to inverse the probabilities of the predictions, like, if we have probability *p* for positives then convert it to *1-p*.\n",
    "+ **AUC = 0.5**: It implies that our predictions are completely random. So, for binary classification, if we predict 0.5 for all targets, we get an AUC of 0.5.\n",
    "+ **AUC < 0.5**: It implies that the model is worse than random. Most of the times, its because we inverted the classes.\n",
    "+ **AUC closer to 1** is considered good.\n",
    "\n",
    "AUC is a widely used metric for skewed binary classification tasks in the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose ROC to get better threshold value to predict the classes out of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Log loss\n",
    "\n",
    "$ \\text{Log_Loss} = -1 * [target * log(prediction) + (1 - target) * log(1-prediction)] $  \n",
    "where, \n",
    "+ target is either 0 or 1\n",
    "+ prediction is the probability of a sample belonging to class 1.\n",
    "\n",
    "For multiple samples in the dataset, the log-loss over all samples is a mere average of all individual log-losses. Log loss penalizes quite high for an incorrect or far-off prediction i.e. log loss punishes you for being sure and very wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_probs):\n",
    "    \"\"\"\n",
    "    Compute the log loss for the given values\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_probs: Predicted probabilities from the model\n",
    "    :returns Log loss over all given values\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    eps = 1e-15\n",
    "    losses = []\n",
    "    for yt, yp in zip(y_true, y_probs):\n",
    "        # Adjust the probability\n",
    "        # 0 is converted to eps or 1e-15\n",
    "        # 1 is converted to 1-eps or 1-1e-15\n",
    "        y_pred = np.clip(yp, eps, 1-eps)\n",
    "        loss = -1.0 * (yt * np.log(y_pred) + (1-yt)*np.log(1-y_pred))\n",
    "        losses.append(loss)\n",
    "    # Mean of all losses\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49882711861432294"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_true, y_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49882711861432294"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(y_true, y_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now, we know important Evaluation metrics those are used for Binary classification problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
