{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics in classification problems\n",
    "\n",
    "We need some kind of evaluation metric to evaluate how good our trained model is doing on unseen data (validation set). Following are the metrics which are commonly used for classification problems:\n",
    "\n",
    "1. Accuracy score\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. Area under ROC (Receiver Operating characteristics) or AUC\n",
    "5. Log loss\n",
    "\n",
    "Knowing above evaluation metrics is good, but we should also know when to use which metric. These metrics usage depend on the problem and mainly target we're trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider, a **binary classification problem** i.e. *the target variable is divided into 2 classes*. An example could be a classification problem for given chest X-ray images to classify if there is pneumothorax in the image. Pneumothorax is a condition where the lung is collapsed and it can be seen in the chest X-ray image.\n",
    "\n",
    "![Normal vs Pneumothorax](https://assets.aboutkidshealth.ca/akhassets/Pneumothorax_XRAY_MEDIMG_PHO_EN.png?RenditionID=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we're given 100 images with equal number of Pneumothorax and non-Pneumothorax images. For our training purpose, we divide the dataset into training and validation sets (7:3) with same ratio as in original dataset.\n",
    "\n",
    "Thus, we'll have 70 images in training set and 30 images in validation set. Then we train our model on 70 images and we'd like to evaluate the model using validation set. Let's look at our Evaluation metrics for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy score\n",
    "\n",
    "Accuracy score is the ratio of correct predictions out of total in the target. Let's say in our above example, out of 30 images in validation set, the model predicts 27 images correctly. Then we can say model predicts with 90% accuracy or 0.9 accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_v1(y_true, y_pred):\n",
    "    \"\"\" Computes the accuracy score\n",
    "    :param y_true - Actual target values\n",
    "    :param y_pred - Predicted values from the model\n",
    "    :returns the accuracy score\n",
    "    \"\"\"\n",
    "    # Assign correct variable to zero, which will contain total correct predictions out of the actual values \n",
    "    correct = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct += 1\n",
    "    return 1.0 * correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the accuracy score for the sample data\n",
    "targets = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "preds   = [0, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "accuracy_score_v1(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that our `accuracy_score_v1` method gives the accuracy score of 0.8 or 80%.\n",
    "\n",
    "`sklearn` python package provides a function to calculate the accuracy score which we can use to cross check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from `sklearn` package's method we get same accuracy score which means our implementation of accuracy score is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider another example, where we are given a dataset of 100 images in which only 10 images has Pneumothorax and rest are non-Pneumothorax. If we divide the dataset into training and validation as 80:20 with equal ratio of images. Then training set will contain 72 images of non-Pneumothorax and 8 images of Pneumothorax. Similarly, validation set will contain 18 images of non-Pneumothorax and 2 images of Pneumothorax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, if we always predict non-Pneumothorax for any image, then still we'll get 90% accuracy without building a model. But would that be a good case? Definitely not. So we can see that having evaluation metric as `accuracy_score` for all problems wouldn't work. Specially, not in the cases where target variable is skewed. Here comes the `precision`, `recall`, `F1-score`, etc. for the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we move forward with Precision and others, we need to be familiar with some terminologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive or TP**: It is defined as if the model predicts `True` where the actual value is also `True`, then consider it as True Positive.  \n",
    "**True Negative or TN**: It is defined as if the model predicts `False` where the actual value is also `False`, then consider it as True Negative.  \n",
    "**False Positive or FP**: It is defined as if the model predicts `True` where the actual value is `False`, then consider it as False Positive.  \n",
    "**False Negative or FN**: It is defined as if the model predicts `False` where the actual value is `True`, then consider it as False Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    \"\"\" Computes count of True Positive\n",
    "    :param y_true - Actual target values\n",
    "    :param y_pred - Predicted values\n",
    "    :return count of true positive\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \"\"\" Computes count of True Negative\n",
    "    :param y_true - Actual target values\n",
    "    :param y_pred - Predicted values\n",
    "    :return count of true negative\n",
    "    \"\"\"\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \"\"\" Computes count of False Positive\n",
    "    :param y_true - Actual target values\n",
    "    :param y_pred - Predicted values\n",
    "    :return count of false positive\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \"\"\" Computes count of False Negative\n",
    "    :param y_true - Actual target values\n",
    "    :param y_pred - Predicted values\n",
    "    :return count of false negative\n",
    "    \"\"\"\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:4\n",
      "TN:4\n",
      "FP:1\n",
      "FN:1\n"
     ]
    }
   ],
   "source": [
    "# Let's take same values above targets and preds.\n",
    "targets = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "preds   = [0, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "# We know that, TP = 4, TN = 4, FP = 1, FN = 1.\n",
    "# Let's see what our functions gives\n",
    "print(f'TP:{true_positive(targets, preds)}')\n",
    "print(f'TN:{true_negative(targets, preds)}')\n",
    "print(f'FP:{false_positive(targets, preds)}')\n",
    "print(f'FN:{false_negative(targets, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!! we have same values as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
