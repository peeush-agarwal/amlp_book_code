{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics in classification problems\n",
    "\n",
    "We need some kind of evaluation metric to evaluate how good our trained model is doing on unseen data (validation set). Following are the metrics which are commonly used for classification problems:\n",
    "\n",
    "1. Accuracy score\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. Area under ROC (Receiver Operating characteristics) or AUC\n",
    "5. Log loss\n",
    "\n",
    "Knowing above evaluation metrics is good, but we should also know when to use which metric. These metrics usage depend on the problem and mainly target we're trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider, a **binary classification problem** i.e. *the target variable is divided into 2 classes*. An example could be a classification problem for given chest X-ray images to classify if there is pneumothorax in the image. Pneumothorax is a condition where the lung is collapsed and it can be seen in the chest X-ray image.\n",
    "\n",
    "![Normal vs Pneumothorax](https://assets.aboutkidshealth.ca/akhassets/Pneumothorax_XRAY_MEDIMG_PHO_EN.png?RenditionID=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we're given 100 images with equal number of Pneumothorax and non-Pneumothorax images. For our training purpose, we divide the dataset into training and validation sets (7:3) with same ratio as in original dataset.\n",
    "\n",
    "Thus, we'll have 70 images in training set and 30 images in validation set. Then we train our model on 70 images and we'd like to evaluate the model using validation set. Let's look at our Evaluation metrics for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy score\n",
    "\n",
    "Accuracy score is the ratio of correct predictions out of total in the target. Let's say in our above example, out of 30 images in validation set, the model predicts 27 images correctly. Then we can say model predicts with 90% accuracy or 0.9 accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_v1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: the accuracy score\n",
    "    \"\"\"\n",
    "    # Assign correct variable to zero, which will contain total correct predictions out of the actual values \n",
    "    correct = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            correct += 1\n",
    "    return 1.0 * correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the accuracy score for the sample data\n",
    "targets = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "preds   = [0, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "accuracy_score_v1(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that our `accuracy_score_v1` method gives the accuracy score of 0.8 or 80%.\n",
    "\n",
    "`sklearn` python package provides a function to calculate the accuracy score which we can use to cross check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from `sklearn` package's method we get same accuracy score which means our implementation of accuracy score is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider another example, where we are given a dataset of 100 images in which only 10 images has Pneumothorax and rest are non-Pneumothorax. If we divide the dataset into training and validation as 80:20 with equal ratio of images. Then training set will contain 72 images of non-Pneumothorax and 8 images of Pneumothorax. Similarly, validation set will contain 18 images of non-Pneumothorax and 2 images of Pneumothorax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, if we always predict non-Pneumothorax for any image, then still we'll get 90% accuracy without building a model. But would that be a good case? Definitely not. So we can see that having evaluation metric as `accuracy_score` for all problems wouldn't work. Specially, not in the cases where target variable is skewed. Here comes the `precision`, `recall`, `F1-score`, etc. for the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we move forward with Precision and others, we need to be familiar with some terminologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive or TP**: It is defined as if the model predicts `True` where the actual value is also `True`, then consider it as True Positive.  \n",
    "**True Negative or TN**: It is defined as if the model predicts `False` where the actual value is also `False`, then consider it as True Negative.  \n",
    "**False Positive or FP**: It is defined as if the model predicts `True` where the actual value is `False`, then consider it as False Positive.  \n",
    "**False Negative or FN**: It is defined as if the model predicts `False` where the actual value is `True`, then consider it as False Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of True Positive\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of true positive\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of True Negative\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of true negative\n",
    "    \"\"\"\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of False Positive\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of false positive\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes count of False Negative\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: count of false negative\n",
    "    \"\"\"\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:4\n",
      "TN:4\n",
      "FP:1\n",
      "FN:1\n"
     ]
    }
   ],
   "source": [
    "# Let's take same values above targets and preds.\n",
    "targets = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "preds   = [0, 1, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "# We know that, TP = 4, TN = 4, FP = 1, FN = 1.\n",
    "# Let's see what our functions gives\n",
    "print(f'TP:{true_positive(targets, preds)}')\n",
    "print(f'TN:{true_negative(targets, preds)}')\n",
    "print(f'FP:{false_positive(targets, preds)}')\n",
    "print(f'FN:{false_negative(targets, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila!! we have same values as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you might be thinking, couldn't we define Accuracy score using terms TP, TN, FP, FN? Yes we can and here's the definition:\n",
    "\n",
    "$ \\text{accuracy_score} = \\dfrac{TP + TN}{TP + TN + FP + FN} $\n",
    "\n",
    "We'll define our new function for `accuracy_score` using these terms internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_v2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: calculated accuracy score from the given values\n",
    "    \"\"\"\n",
    "    \n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    return (tp + tn) / (tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score (v1)     : 0.8\n",
      "Acc score (v2)     : 0.8\n",
      "Acc score (sklearn): 0.8\n"
     ]
    }
   ],
   "source": [
    "print(f'Acc score (v1)     : {accuracy_score_v1(targets, preds)}')\n",
    "print(f'Acc score (v2)     : {accuracy_score_v2(targets, preds)}')\n",
    "print(f'Acc score (sklearn): {accuracy_score(targets, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's deep dive into other Evaluation metrics for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precision vs Recall\" title=\"Precision vs Recall\" style=\"height:750px\" align=\"right\" />\n",
    "*Mathematically, Precision is defined as the fraction of relevant instances among all retrieved instances.*\n",
    "\n",
    "In simple terms, it can be defined as the ratio of correctly predicted positives out of total predicted positives. \n",
    "\n",
    "$ Precision = \\dfrac{TP}{TP+FP} $\n",
    "\n",
    "This metric and others as well are helpful in case of skewed target variable. Considering an example above, our accuracy was 90% though the Precision would be 0, because there are no True Positive found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the precision score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: Precision score for the given values\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    return 1.0*tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8\n",
      "Precision: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Consider another example to actually show power of Precision over Accuracy score\n",
    "targets = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "preds   = [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "print('Accuracy : {}'.format(accuracy_score_v2(targets, preds)))\n",
    "print('Precision: {}'.format(precision(targets, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our accuracy is 80% but our precision is just 50% and this conveys that our model is having issues with predicting Positives correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recall\n",
    "\n",
    "*Mathematically, it is defined as the fraction of retrieved instances among all relevant instances.*\n",
    "\n",
    "It can be defined as the ratio of correctly predicted positives out of all actual positives.\n",
    "\n",
    "$ Recall = \\dfrac{TP}{TP+FN} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the recall score\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns: Recall score for the given values\n",
    "    \"\"\"\n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    return 1.0*tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8\n",
      "Precision: 0.5\n",
      "Recall   : 0.5\n"
     ]
    }
   ],
   "source": [
    "# Consider another example to actually show power of Precision, Recall over Accuracy score\n",
    "targets = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "preds   = [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "print('Accuracy : {}'.format(accuracy_score_v2(targets, preds)))\n",
    "print('Precision: {}'.format(precision(targets, preds)))\n",
    "print('Recall   : {}'.format(recall(targets, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
