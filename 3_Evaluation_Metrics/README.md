# Evaluation metrics

We need some metrics to evaluate our trained models to verify how they might perform on unseen data. In this section, we've looked at metrics for classification.

## Classification evaluation metrics

### [Binary classification](1_Binary_Classification_Evaluation_Metrics.ipynb)

1. Accuracy score
1. Precision
1. Recall
1. F1-score
1. Area under Receiver Operating Characteristic (ROC), *in short AUC*
1. Log loss

### [Multi-class classification](2_Multiclass_Classification_Evaluation_Metrics.ipynb)

1. Macro-averaged
   1. Precision
   1. Recall
   1. F1-score
1. Micro-averaged
   1. Precision
   1. Recall
   1. F1-score
1. Weighted-averaged
   1. Precision
   1. Recall
   1. F1-score

### [Multi-label classification](3_Multi_Label_Classification_Evaluation_Metrics.ipynb)

1. Precision at k (P@k)
1. Average precision at k (AP@k)
1. Mean average precision at k (MAP@k)
1. Mean column-wise log loss