{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for Multi-class classification problems\n",
    "\n",
    "We can easily extend our knowledge about [evaluation metrics for Binary classification problems](1_Binary_Classification_Evaluation_Metrics.ipynb) we learnt already, which were:\n",
    "+ Accuracy score\n",
    "+ Precision\n",
    "+ Recall\n",
    "+ AUC\n",
    "+ Log loss\n",
    "\n",
    "First, what is **Multi-class classification** problems?  \n",
    "In Binary classification problems, we were classifying a sample into 2 classes (Pneumothorax or non-Pneumothorax, etc.). Here, we'll classify a given sample out of more than 2 classes. For example, in IRIS dataset, we need to classify a sample out of 3 species, namely Setosa, Versicolour and Virginica.\n",
    "\n",
    "**What are the different Evaluation metrics for these type of problems?**  \n",
    "Let me tell you that the concepts like Precision, Recall, etc. remains same and we just use them and compute average or weighted average, etc. So, following are the metrics for Precision:\n",
    "+ Macro averaged precision\n",
    "+ Micro averaged precision\n",
    "+ Weighted precision\n",
    "\n",
    "Similar concepts are present for Recall, F1-Score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deep-dive into Precision related evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro averaged precision\n",
    "\n",
    "It is defined as compute precision for all classes individually and then average them.  \n",
    "This can be better understand in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from eval_metrics import * # Python file that stores all functions created in Binary_Classification Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_average_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Macro average precision for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns macro-averaged precision for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of classes in y_true\n",
    "    classes = list(np.unique(y_true))\n",
    "    \n",
    "    precision = 0\n",
    "    \n",
    "    for class_ in classes:\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        temp_precision = tp/(tp+fp)\n",
    "        precision += temp_precision\n",
    "    \n",
    "    precision /= len(classes)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro average precision\n",
    "\n",
    "It is defined as calculate classwise TP and FP and then use that to calculate overall precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_average_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Micro average precision for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns micro-averaged precision for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = list(np.unique(y_true))\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for class_ in classes:\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        temp_tp = true_positive(temp_true, temp_pred)\n",
    "        temp_fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        tp += temp_tp\n",
    "        fp += temp_fp\n",
    "    \n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted precision\n",
    "\n",
    "It is defined same as macro but in this case, it is weighted average depending on the number of items in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Weighted average precision for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns weighted-averaged precision for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    cnt_classes = Counter(y_true)\n",
    "    \n",
    "    precision = 0\n",
    "    \n",
    "    for class_, cnt in cnt_classes.items():\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        temp_precision = tp / (tp + fp)\n",
    "        \n",
    "        precision += temp_precision * cnt\n",
    "    \n",
    "    return precision / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our implementations with `sklearn` package results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    Precision: 0.3611111111111111\n",
      "Micro    Precision: 0.4444444444444444\n",
      "Weighted Precision: 0.39814814814814814\n"
     ]
    }
   ],
   "source": [
    "targets = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "preds   = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "\n",
    "print(f'Macro    Precision: {macro_average_precision(targets, preds)}')\n",
    "print(f'Micro    Precision: {micro_average_precision(targets, preds)}')\n",
    "print(f'Weighted Precision: {weighted_average_precision(targets, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    Precision: 0.3611111111111111\n",
      "Micro    Precision: 0.4444444444444444\n",
      "Weighted Precision: 0.39814814814814814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(f'Macro    Precision: {precision_score(targets, preds, average=\"macro\")}')\n",
    "print(f'Micro    Precision: {precision_score(targets, preds, average=\"micro\")}')\n",
    "print(f'Weighted Precision: {precision_score(targets, preds, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
