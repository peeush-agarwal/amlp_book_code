{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for Multi-class classification problems\n",
    "\n",
    "We can easily extend our knowledge about [evaluation metrics for Binary classification problems](1_Binary_Classification_Evaluation_Metrics.ipynb) we learnt already, which were:\n",
    "+ Accuracy score\n",
    "+ Precision\n",
    "+ Recall\n",
    "+ AUC\n",
    "+ Log loss\n",
    "\n",
    "First, what is **Multi-class classification** problems?  \n",
    "In Binary classification problems, we were classifying a sample into 2 classes (Pneumothorax or non-Pneumothorax, etc.). Here, we'll classify a given sample out of more than 2 classes. For example, in IRIS dataset, we need to classify a sample out of 3 species, namely Setosa, Versicolour and Virginica.\n",
    "\n",
    "**What are the different Evaluation metrics for these type of problems?**  \n",
    "Let me tell you that the concepts like Precision, Recall, etc. remains same and we just use them and compute average or weighted average, etc. So, following are the metrics for Precision:\n",
    "+ Macro averaged precision\n",
    "+ Micro averaged precision\n",
    "+ Weighted precision\n",
    "\n",
    "Similar concepts are present for Recall, F1-Score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deep-dive into Precision related evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro averaged precision\n",
    "\n",
    "It is defined as compute precision for all classes individually and then average them.  \n",
    "This can be better understand in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from eval_metrics import * # Python file that stores all functions created in Binary_Classification Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_averaged_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Macro averaged precision for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns macro-averaged precision for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of classes in y_true\n",
    "    classes = list(np.unique(y_true))\n",
    "    \n",
    "    precision = 0\n",
    "    \n",
    "    for class_ in classes:\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        temp_precision = tp/(tp+fp)\n",
    "        precision += temp_precision\n",
    "    \n",
    "    precision /= len(classes)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro averaged precision\n",
    "\n",
    "It is defined as calculate classwise TP and FP and then use that to calculate overall precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_averaged_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Micro averaged precision for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns micro-averaged precision for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = list(np.unique(y_true))\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for class_ in classes:\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        temp_tp = true_positive(temp_true, temp_pred)\n",
    "        temp_fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        tp += temp_tp\n",
    "        fp += temp_fp\n",
    "    \n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted precision\n",
    "\n",
    "It is defined same as macro but in this case, it is weighted average depending on the number of items in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_averaged_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Weighted averaged precision for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns weighted-averaged precision for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    cnt_classes = Counter(y_true)\n",
    "    \n",
    "    precision = 0\n",
    "    \n",
    "    for class_, cnt in cnt_classes.items():\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        temp_precision = tp / (tp + fp)\n",
    "        \n",
    "        precision += temp_precision * cnt\n",
    "    \n",
    "    return precision / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our implementations with `sklearn` package results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    Precision: 0.3611111111111111\n",
      "Micro    Precision: 0.4444444444444444\n",
      "Weighted Precision: 0.39814814814814814\n"
     ]
    }
   ],
   "source": [
    "targets = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "preds   = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "\n",
    "print(f'Macro    Precision: {macro_averaged_precision(targets, preds)}')\n",
    "print(f'Micro    Precision: {micro_averaged_precision(targets, preds)}')\n",
    "print(f'Weighted Precision: {weighted_averaged_precision(targets, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    Precision: 0.3611111111111111\n",
      "Micro    Precision: 0.4444444444444444\n",
      "Weighted Precision: 0.39814814814814814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(f'Macro    Precision: {precision_score(targets, preds, average=\"macro\")}')\n",
    "print(f'Micro    Precision: {precision_score(targets, preds, average=\"micro\")}')\n",
    "print(f'Weighted Precision: {precision_score(targets, preds, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "Let's define functions for Multi-class Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro-averaged Recall\n",
    "\n",
    "It is defined as the overall average of recalls computed for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_averaged_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Macro averaged recall for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns macro-averaged recall for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = list(np.unique(y_true))\n",
    "    recall = 0.0\n",
    "    for class_ in classes:\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        temp_recall = tp/(tp+fn)\n",
    "        recall += temp_recall\n",
    "    \n",
    "    return recall/len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro-averaged Recall\n",
    "\n",
    "It is defined as the computed recall for overall TP and FN for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_averaged_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Micro averaged recall for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns micro-averaged recall for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = list(np.unique(y_true))\n",
    "    \n",
    "    tp = 0.0\n",
    "    fn = 0.0\n",
    "    for class_ in classes:\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        fn += false_negative(temp_true, temp_pred)\n",
    "    \n",
    "    return tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted-averaged Recall\n",
    "\n",
    "It is same as Macro-averaged recall, but the average is weighted as per items in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_averaged_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Weighted averaged recall for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns weighted-averaged recall for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes_cnt = Counter(y_true)\n",
    "    \n",
    "    recall = 0.0\n",
    "    for class_, cnt in classes_cnt.items():\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        \n",
    "        temp_recall = tp/(tp+fn)\n",
    "        recall += temp_recall * cnt\n",
    "    return recall/len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our implementation with `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    Recall: 0.4166666666666667\n",
      "Micro    Recall: 0.4444444444444444\n",
      "Weighted Recall: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "targets = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "preds   = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "\n",
    "print(f'Macro    Recall: {macro_averaged_recall(targets, preds)}')\n",
    "print(f'Micro    Recall: {micro_averaged_recall(targets, preds)}')\n",
    "print(f'Weighted Recall: {weighted_averaged_recall(targets, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    Recall: 0.4166666666666667\n",
      "Micro    Recall: 0.4444444444444444\n",
      "Weighted Recall: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(f'Macro    Recall: {recall_score(targets, preds, average=\"macro\")}')\n",
    "print(f'Micro    Recall: {recall_score(targets, preds, average=\"micro\")}')\n",
    "print(f'Weighted Recall: {recall_score(targets, preds, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! We've got same results from package implementation as with our own implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is F1-score for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro-averaged F1 score\n",
    "\n",
    "It is defined as the overall average of all computed F1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_averaged_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Macro averaged F1-score for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns macro-averaged F1-score for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = list(np.unique(y_true))\n",
    "    \n",
    "    f1_score = 0.0\n",
    "    \n",
    "    for class_ in classes:\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        precision_ = precision(temp_true, temp_pred)\n",
    "        recall_ = recall(temp_true, temp_pred)\n",
    "        \n",
    "        try:\n",
    "            temp_f1 = (2*precision_*recall_) / (precision_ + recall_)\n",
    "            f1_score += temp_f1\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        \n",
    "    return f1_score/len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro-averaged F1-score\n",
    "\n",
    "It is defined as the overall F1-score for all classes precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_averaged_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Micro averaged F1-score for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns micro-averaged F1-score for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    precision_ = micro_averaged_precision(y_true, y_pred)\n",
    "    recall_ = micro_averaged_recall(y_true, y_pred)\n",
    "    \n",
    "    return (2*precision_*recall_) / (precision_ + recall_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted-averaged F1-score\n",
    "\n",
    "It is same as Macro-averaged F1-score but with the weighted average as items in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Weighted averaged F1-score for Multi-class classification problem.\n",
    "    \n",
    "    :param y_true: Actual target values\n",
    "    :param y_pred: Predicted values from the model\n",
    "    :returns weighted-averaged F1-score for given values.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes_cnt = Counter(y_true)\n",
    "    \n",
    "    f1_score = 0.0\n",
    "    for class_, cnt in classes_cnt.items():\n",
    "        temp_true = [1 if yt == class_ else 0 for yt in y_true]\n",
    "        temp_pred = [1 if yp == class_ else 0 for yp in y_pred]\n",
    "        \n",
    "        precision_ = precision(temp_true, temp_pred)\n",
    "        recall_ = recall(temp_true, temp_pred)\n",
    "        \n",
    "        try:\n",
    "            temp_f1_score = (2*precision_*recall_) / (precision_ + recall_)\n",
    "            f1_score += temp_f1_score*cnt\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "    return f1_score / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with `sklearn` package implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    F1-score: 0.38095238095238093\n",
      "Micro    F1-score: 0.4444444444444444\n",
      "Weighted F1-score: 0.41269841269841273\n"
     ]
    }
   ],
   "source": [
    "targets = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "preds   = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "\n",
    "print(f'Macro    F1-score: {macro_averaged_f1_score(targets, preds)}')\n",
    "print(f'Micro    F1-score: {micro_averaged_f1_score(targets, preds)}')\n",
    "print(f'Weighted F1-score: {weighted_f1_score(targets, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro    F1-score: 0.38095238095238093\n",
      "Micro    F1-score: 0.4444444444444444\n",
      "Weighted F1-score: 0.41269841269841273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f'Macro    F1-score: {f1_score(targets, preds, average=\"macro\")}')\n",
    "print(f'Micro    F1-score: {f1_score(targets, preds, average=\"micro\")}')\n",
    "print(f'Weighted F1-score: {f1_score(targets, preds, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
